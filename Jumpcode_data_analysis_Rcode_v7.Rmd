---
title: "Analysis with 10x Data Using DepleteX (Cell Ranger >= v7)"
output:
  github_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
engine: r
engine.opts:
  memory: 8192
date: "2023-09-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.path = "jumpcode_pbmc_analysis_files/figure-gfm/"
)
```

# Plotting median UMIs/cell and Genes/cell

We first want to assess some basic metrics such as UMIs/cell and Genes/cell to assess the gain in transcriptomic read information of untargeted genes.

You can use a directory containing the barcodes.tsv, genes.tsv, and matrix.mtx file as output from CellRanger or filtered_feature_barcode_matrix.h5 file from CellRanger

You will also need the gene target list provided by Jumpcode Genomics. We experimentally remove UMIs associated with the targeted transcripts. We want to evaluate how reads are being redistributed for genes that have not been targeted with CRISPR. Thus, we should see a boost in both throughput for the remaining untargeted transcriptome

First we will load all necessary packages

```{r load necessary packages}
x <- c("Seurat", "dplyr", "patchwork", "warrenlabSC", "ggplot2", "ggpubr", "rstatix", "plotrix", "ggExtra", "cowplot", "tidyr", "purrr", "scuttle")

# Use lapply with suppressPackageStartupMessages and library
invisible(lapply(x, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))
```


Here we are going to create a dataframe containing all the throughput information (umis/cell and genes/cell) using the following function:

```{r creating dataframe for UMIs and Genes per cell}

#matrix = PATH to .h5 file or PATH to directory containing barcodes.tsv, genes.tsv, and matrix.mtx from CellRanger
#gene list = gene list provided by Jumpcode containing targets
#sample = whichever name you would like to name the sample
#meta_data = optional argument if using a meta data csv file to add
#project_name = optional argument to what you would like to name the project


umis_genes_per_cell <- function(matrix, gene_list, sample, project_name = NULL, meta_data = NULL) {
    if (length(matrix) > 1) {
  list.mtx <- lapply(matrix, FUN = function(x) {
    if (file.exists(x)) {
      if (file.info(x)$isdir) {
        Read10X(x)
      } else {
        Read10X_h5(x)
      }
    } else {
      stop(paste("File or directory", x, "does not exist."))
    }
  })
} else if (length(matrix) == 1) {
  x <- matrix[[1]]
  if (file.exists(x)) {
    if (file.info(x)$isdir) {
      list.mtx <- Read10X(x)
    } else {
      list.mtx <- Read10X_h5(x)
    }
  } else {
    stop(paste("File or directory", x, "does not exist."))
  }
} else {
  stop("No matrix provided.")
}

      # Check if gene_list file exists and read it
    if (file.exists(gene_list)) {
      targets <- read.delim(gene_list, header = FALSE) # Assuming it's a text file with one column
      targets <- targets$V1
    } else {
      stop(paste("Gene list file", gene_list, "does not exist."))
    }
    
    #first find common barcodes and genes
    
    if(exists("list.mtx")) {
      print("successfully read 10x files")
    } else {
      stop("failed to add matrices")
    }
    
    #remove the targeted genes from the control matrix file
    if (!is.list(list.mtx)) {
      list.mtx <- list.mtx[!rownames(list.mtx) %in% targets, ]
      list.mtx <- CreateSeuratObject(list.mtx, project = sample)
      print("successfully created seurat object")
      if (any(sapply(colnames(list.mtx), function(colname) any(grepl("[2-9][0-9]*", colname))))) {
        meta <- read.csv(meta_data)
        list.mtx <- add.meta.data(list.mtx, meta)
        df <- list.mtx@meta.data
      } else {
        list.mtx$replicate <- as.character(seq_along(list.mtx))
        df <- list.mtx@meta.data
      }
      
    } else {
      for (i in seq_along(list.mtx)) {
      list.mtx[[i]] <- list.mtx[[i]][!rownames(list.mtx[[i]]) %in% targets, ]
        }
      #create seurat object
      list.mtx <- lapply(list.mtx, FUN = function (x) {
      x <- CreateSeuratObject(x, project = sample)
      })
      print("successfully created seurat object")
      c.combined <- merge(list.mtx[[1]], y = list.mtx[-1], add.cell.ids =
                          as.character(seq_along(list.mtx)),project= project_name)
      c.combined$replicate <- gsub(x = colnames(c.combined), pattern = ".{19}$",replacement = "")
          df <- c.combined@meta.data
    }
}
```


```{r create dataframe for plotting}
#create a dataframe using umis_genes_per_cell function to calculate the total throughput with untargeted genes

df <- umis_genes_per_cell(matrix = "cellranger_aggr_pbmc/control_depletex_pbmc_triplicates.h5", gene_list = "targets_files/all_targets_with_nvg_workflow.txt", sample = "PBMC", meta_data = "cellranger_aggr_pbmc/aggregation.csv")
```


Now we can calculate the median umis/cell and genes per cell and plot the distribution of each

```{r calculate median umis and genes per cell}
#calculate the medians for counts and genes per cell
medians <- df %>% dplyr::group_by(depletion) %>% get_summary_stats(type = "median")

#median umis
median_umi <- medians %>% dplyr::filter(variable == "nCount_RNA")
median_umi$max <- max(df$nCount_RNA)

#median genes
median_genes <- medians %>% dplyr::filter(variable == "nFeature_RNA")
median_genes$max <- max(df$nFeature_RNA)
```


```{r plot umis per cell, fig.height=4, fig.width=8}
p1 <- ggviolin(df, x = "depletion", y = "nCount_RNA", xlab = F, ylab= "UMIs/cell", fill = "depletion", add = "boxplot", add.params = list(fill = "white"), order = c("10x_v3","DepleteX")) + 
  scale_y_log10() +
  geom_text(data = median_umi, aes(depletion, max, label = round(median, 1)), position = position_dodge(width = 0.8), size = 4, vjust = -0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.title.x = element_blank(), legend.title = element_blank()) +
  NoLegend()


p2 <- ggplot(df, aes(x =nCount_RNA , fill=depletion, after_stat(scaled))) + 
  geom_density(alpha=0.2) + 
  xlab("UMIs/cell") + 
  ylab("Scaled Density") + 
  scale_x_log10() + 
  geom_vline(data = median_umi, aes(xintercept = median, color=depletion), linetype='dashed') +
  theme_classic()

p1+p2

```


```{r plot genes per cell, fig.height=4, fig.width=8}

p3 <- ggviolin(df, x = "depletion", y = "nFeature_RNA", xlab = F, ylab= "Genes/cell", fill = "depletion", add = "boxplot", add.params = list(fill = "white"), order = c("10x_v3","DepleteX")) +
  geom_text(data = median_genes, aes(depletion, max-100, label = round(median, 1)), position = position_dodge(width = 0.8), size = 4, vjust = -0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.title.x = element_blank(), legend.title = element_blank())

p4 <- ggplot(df, aes(x =nFeature_RNA , fill=depletion, after_stat(scaled))) + 
  geom_density(alpha=0.2) + 
  xlab("nGenes/cell") + 
  ylab("Scaled Density") + 
  geom_vline(data = median_genes, aes(xintercept = median, color=depletion), linetype='dashed') + 
  theme_classic()

p3+p4

```


Here we can see a boost in total throughput of untargeted genes with DepleteX via increase in both UMIs/cell and Genes/cell.

# Setup the seurat object

We are going to use the Seurat toolkit to perform all subsequent downstream analysis for this tutorial.

We start by reading the data using the Read10x_h5() function which reads the .h5 file containing the Seurat inputs generated by CellRanger and generates a count matrix. We next use the count matrix to create a seurat object.


```{r create count matrices}
#create matrix
mtx <- Read10X_h5("cellranger_aggr_pbmc/control_depletex_pbmc_triplicates.h5")
```

We are next going to create the Seurat object. We used Cell Ranger Aggregate to combine ~30K cells processed with the 10x v3.1 workflow and ~30k cells processed with DepleteX so that we are working with a single .h5 file. Alternatively, you can load in each dataset separately if you did not use Cell Ranger Aggregate. Later, we will use data-driven techniques to determine appropriate thresholds for removing low-quality cells.

```{r Create Seurat object}
#create the seurat object
so <- CreateSeuratObject(mtx, project = "PBMC")
```


We can also add the metadata that was used in Cell Ranger Aggregate to the Seurat object

```{r add metadata to seurat object}

#import meta data
meta_data <- read.csv("cellranger_aggr_pbmc/aggregation.csv")
so <- add.meta.data(seurat = so, meta.data = meta_data)

#we can remove the raw matrix which is now stored in the Seurat object to save some memory
rm(mtx)
```


Next we calculate some key QC metrics to verify DepleteX depletion efficiency

```{r calculate qc metrics}

#calculate percent mito content 
so <- PercentageFeatureSet(so, pattern = "MT-", col.name = 'percent.mt')

#calculate percent ribo
so <- PercentageFeatureSet(so, pattern = "^RP[SL][[:digit:]]|^RPLP[[:digit:]]|^RPSA", col.name = 'percent.rb')

#create regex for target list
nvg <- read.delim("targets_files/nvg_genes.txt", header = F)
nvg <- nvg$V1
nvg <- paste0("^", nvg, "$", collapse = "|")

#percent nvg genes
so <- PercentageFeatureSet(so, pattern = nvg, col.name = 'percent.nvg')
```


Now we can use a boxplot to visualize how well we are removing UMIs associated with certain targeted sets, namely mitochondrial, ribosomal, and NVG

```{r create boxplots}

#select columns for plotting
selected_columns <- c("percent.mt", "percent.rb", "percent.nvg", "depletion")

#create temp data frame
temp <- so@meta.data %>% select(all_of(selected_columns))

# Reshape the data to a longer format
temp_long <- temp %>%
  pivot_longer(cols = c(percent.mt, percent.rb, percent.nvg),
               names_to = "variable", values_to = "value")

# Calculate the median values for each variable by "depletion" group
medians <- temp %>%
  group_by(depletion) %>%
  summarise(across(c("percent.mt", "percent.rb", "percent.nvg"), median))

# Create a data frame for geom_text() annotations
text_df <- medians %>%
  pivot_longer(cols = c("percent.mt", "percent.rb", "percent.nvg"),
               names_to = "variable", values_to = "median_value")

# Create the ggboxplot and add median value annotations
ggboxplot(data = temp_long, x = "depletion", y = "value", color = "variable", palette = "npg",bxp.errorbar = T, bxp.errorbar.width = 0.2, outlier.shape = NA, facet.by = "variable", order = c("10x_v3","DepleteX")) + 
  ylab("% UMIs/cell") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.title.x = element_blank(), legend.title = element_blank()) +
    geom_text(data = text_df, aes(depletion, 65, label = round(median_value, 1)), 
              position = position_dodge(width = 0.8), size = 8, vjust = -0.5)

```


```{r show line of fit}

line_of_fit <- function(df, x, y, targets = NULL) {
  x <- enquo(x)
  y <- enquo(y)
  
  ggplot(df, aes(x = !!x, y = !!y)) + 
    geom_point(
      aes(
        color = ifelse(!!y > !!x, "Above Unity",
                       ifelse(rownames(df) %in% targets, "Targeted Genes", "Below Unity"))
      ),
      size = 1
    ) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black", linewidth = 1) +
    scale_color_manual(
      values = c(
        "Above Unity" = "#E7B800",
        "Below Unity" = "#00AFBB",
        "Targeted Genes" = "#FF0000"  # Adjust the color as needed
      )
    ) +
    labs(color = "Gene Position") +
    stat_cor(
      method = "pearson",
      aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")),
      label.x = 0.1,
      label.y = 5,
      r.digits = 3
    ) +
    theme_half_open()
}

#aggregate the expression of all replicates
agg <- AggregateExpression(so, group.by = 'sample_id', slot = 'counts', return.seurat = F, assays = 'RNA')

pseudo_bulk <- data.frame(Control = rowMeans(log10(agg$RNA[, 4:6] + 1)),
                                     DepleteX = rowMeans(log10(agg$RNA[, 1:3] + 1)))
pseudo_bulk$gene <- rownames(pseudo_bulk)

targets <- read.csv("targets_files/all_targets_with_nvg_workflow.txt", header = F)
targets <- targets$V1
line_of_fit(df = pseudo_bulk, x = Control, y = DepleteX, targets = targets)
```

For the subsequent downstream analysis, we want to analyze the control (10x v3) and depleted (DepleteX) samples separately as they represent two different library compositions

```{r split object}
#splitting the seurat object
so <- SplitObject(so, split.by = "depletion")
```


# Data pre-processing and QC workflow

The steps below demonstrate an example of data pre-processing and QC for scRNA-seq data generated with 10x-v3 and DepleteX.

## Filtering low-quality cells

We are going to use a data-driven technique to evaluate the number of unique genes and UMIs detected in each cell.

1. Dead or decaying cells typically have low complexity with a high % of UMIs contributed by mitochondrial genes
2. Empty droplets or low-quality cells typically express few genes and have low library complexity.
2. Doublets exhibit higher UMI and gene counts which affects downstream interpretation.
3. The total number of genes detected corresponds linearly with unique genes. Over-sequenced cells will have a disruption in this linearity.

### Filtering cells with low throughput

Cells with low throughput can confound biological interpretation. Here we are removing cells with genes expressing less than ~250 genes. Alternatively, cells expressing a min # of genes can be used when creating the Seurat object by evoking the "min.features" argument.

```{r scatter plot of cell complexity}

# Loop through each element in aggr.nvg
for (i in names(so)) {
  current_data <- so[[i]]@meta.data
  
  # Create the first plot (p1)
  plot <- ggplot(current_data, aes(x=log10(nCount_RNA), y=log10(nFeature_RNA))) +
    geom_point(aes(color=depletion)) +
    geom_smooth(method="lm") +
    geom_hline(yintercept = 2.4, colour = "green", linetype = 2) + # removing cells expressing < ~250 genes
    ylim(2,4.5) +
    ggtitle(paste(i)) +
    NoLegend()
  ggMarginal(plot, type = "histogram", fill = "lightgrey")
  
  print(plot)
}
```


```{r remove low sequence-depth cells}

#filter the cells
cells.keep <- list()
for (name in names(so)) {
  current_data <- so[[name]]@meta.data
  cells.keep[[name]] <- rownames(current_data %>% filter(log10(nFeature_RNA) > 2.4))
  so[[name]] <- so[[name]][, cells.keep[[name]]]
}
```


### Filtering compromised cells

Here we are applying a similar application as outlined in the miQC package to interact directly with a Seurat object (https://doi.org/10.1371/journal.pcbi.1009290)

We are using the flexmix package to build a data-driven QC metric that jointly models both proportion of reads that map to mitochondrial genes and the # of uniquely detected genes (nFeature_RNA). From there, we are setting a posterior probability threshold to preserve high quality cells without setting an arbitrary mitochondrial proportion threshold. This helps to preserve data quality for datasets with a particularly high proportion of mitochondrial reads. For example, cancer datasets with high mitochondrial reads are particularly sensitive to mitochondrial filtering where thresholds can either be too stringent by setting a 10% threshold. Additionally, filtering cells based on MAD can sometimes not remove any cells if no significant outliers can be detected.

Nonetheless, if you find that you have a dataset with high quality cells with few outliers, then in some cases, building a flexmix model can fail. If this happens, you can resort to using MAD to filter the low-quality cells.

```{r flexmix, fig.height=4, fig.width=8}

#load flexmix
library(flexmix)
model <- list()
for (i in names(so)) {
  #load the meta data from the seurat object
  current_data <- so[[i]]@meta.data
  #build the flexmix model
  model[[i]] <- flexmix(percent.mt ~ nFeature_RNA, data = current_data, k = 2)
  #extract intercepts from the model
  intercept1 <- parameters(model[[i]], component = 1)[1]
  intercept2 <- parameters(model[[i]], component = 2)[1]
  if (intercept1 > intercept2) {
    compromised_dist <- 1
    intact_dist <- 2
  }
  else {
    compromised_dist <- 2
    intact_dist <- 1
  }
  #extract posterior probabilities
  post <- posterior(model[[i]])
  prob_compromised <- post[, compromised_dist]
  keep <- prob_compromised <= 0.95 #change this to control probability threshold for cells to keep
  #create new data frame with probability metrics
  metrics <- cbind(current_data, prob_compromised = prob_compromised, 
    keep = keep)
  #store predictions of percent mito from the fitted model
  predictions <- fitted(model[[i]])[, intact_dist]
  metrics$intact_prediction <- predictions
  #if the experimental < the prediction, keep those cells
  metrics[metrics$percent.mt < metrics$intact_prediction, 
    ]$keep <- TRUE
  #this removes some high quality cells that sometimes have high probabilities
  min_discard <- min(metrics[!metrics$keep, ]$percent.mt)
  min_index <- which(metrics$percent.mt == min_discard)[1]
  lib_complexity <- metrics[min_index, ]$nFeature_RNA
  #store binary classification
  metrics[metrics$nFeature_RNA <= lib_complexity & metrics$percent.mt >= 
      min_discard, ]$keep <- FALSE
  so[[i]]$keep_cells <- metrics$keep
  #plot the mitochondrial reads
  p <- ggplot(metrics, aes(x = nFeature_RNA, y = percent.mt, 
  colour = keep)) + labs(x = "Unique genes found", y = "Percent reads mitochondrial", 
  color = "Keep") + scale_color_manual(values = c("#999999", "#E69F00")) + 
  geom_point() +
  ggtitle(paste(i))
  print(p)
}
  
```


```{r remove low-quality cells from mito filtering}
#Filter the low-quality cells from the data
cells.keep <- list()
for (i in names(so)) {
so[[i]] <- so[[i]][, so[[i]]@meta.data[, "keep_cells"] == TRUE]
}
```

### Filtering low-complexity cells

For this workflow, we are going to perform another QC filtering based on complexity or the ratio of unique genes/UMI. Typically, low quality cells will have a smaller ratio of unique genes/UMIs. As previously mentioned, the number of genes/cell should scale with an increase in sequencing depth. As a result, cells with a library complexity outside the expected ratio are deemed lower-quality and should be removed for QC.

Due to the linear relationship between the log10(UMI) and log10(gene counts), we can use a linear model to calculate the residuals in relation to the regression line. For this example, we are going to exclude cells that have residuals with > 40% variance below the linear regression to exclude low complexity cells. Keep in mind that this threshold can change depending on the cell-type and experiment.

```{r Find low complexity cells}
lm_list <- list()
for (i in names(so)) {
  lm_list[[i]] = lm(data = so[[i]]@meta.data, formula = log10(nFeature_RNA) ~ log10(nCount_RNA))
  so[[i]]@meta.data$residuals <- residuals(lm_list[[i]])
  so[[i]]@meta.data <- so[[i]]@meta.data %>% mutate(complexity = ifelse(test = so[[i]]@meta.data$residuals >= -0.4, yes = "high" , no = "low"))
  
  p2 <- ggplot(so[[i]]@meta.data, aes(x=log10(nCount_RNA), y=log10(nFeature_RNA))) + geom_point(aes(color = complexity)) + 
    geom_abline(intercept = lm_list[[i]]$coefficients[1] - 0.4 , slope = lm_list[[i]]$coefficients[2], color="orange", linetype=2) + 
    geom_smooth(method="lm") +  # Group by "depletion" column
    ggtitle(paste(i))
  ggMarginal(p2, type = "histogram", fill="lightgrey")

  print(p2)
}

```


```{r Filter the low complexity cells}
# Loop through each element in the Seurat object to filter low complexity cells
for (i in names(so)) {
  # Filter the data directly
  so[[i]] <- so[[i]][, so[[i]]@meta.data[, "complexity"] == 'high']
}
```


Next, we are going to use the doublet removal toolkit scDblFinder (Germain et. al., 2022) to remove doublets from the data. This tool uses a machine-learning algorithm to simulate artificial doublets from the data. From there, real cells get assigned a doublet score probability to which we will filter cells called as doublets. 

This workflow can be done either including or exluding clustering information. We find that the tool works well without performing clustering prior to artificial doublet information which can conserve time. However, if you have a ground truth cell identities, then using clustering information can be useful for simulating artificial doublets in the data for removal.



### Doublet filtering

```{r doublet filtering}
#load libraries
library(scDblFinder)
library(SingleCellExperiment)

#natural log normalize and scale the raw counts data
so <- lapply(X = so, FUN = function(x) {
  x <- NormalizeData(x, assay = 'RNA')
})

sce <- list()
for (i in names(so)) {
  sce[[i]] <- as.SingleCellExperiment(so[[i]], assay = "RNA")
  
  message("finished creating sce object...")
  
  sce[[i]] <- scDblFinder(sce[[i]], clusters = NULL, samples = "sample_id", multiSampleMode = "split") #using the samples argument is important if multiple captures were used
  
  message("finished simulating doublets...")
  
  #save to original seurat object
  so[[i]]$class <- sce[[i]]$scDblFinder.class
  so[[i]]$class <- factor(so[[i]]$class, levels = c("singlet","doublet"))
  
  message("finished adding to seurat object...")
  
  #remove sce object
  if (!length(sce) == length(so)) {
    message("processing next sample...")
  } else {
      rm(sce)
  }
}
```

After simulating the doublets, we can visualize the results. Called doublets should have ~2x the amount of counts. Additionally, heterotypic doublets will have a higher amount of complexity due to mixing of cell type specific genes which can be visualized as well. 

```{r plot doublet metrics}
#plot doublets
for (i in names(so)) {
  
  p1 <- VlnPlot(so[[i]], features = c("nCount_RNA", "nFeature_RNA"), group.by = "class", split.by = "channel") & xlab(NULL) & ggtitle(i)
  p2 <- FeatureScatter(so[[i]], feature1 = "nCount_RNA", feature2 = "nFeature_RNA", group.by = "class") + ggtitle(i)
  plot(p1) + plot(p2)
  
  #verify counts are roughly twice as much
  print(so[[i]]@meta.data %>% group_by(class) %>% summarise(umi_count = mean(nCount_RNA)))
}
```

Now we can filter the data to only include called singlets

```{r remove the called doublets}
# Loop through each element and remove the called doublets
for (i in names(so)) {
  # Filter the data directly using dplyr's filter function
  so[[i]] <- so[[i]][, so[[i]]@meta.data[, "class"] == 'singlet']
}
```


## Cell cycle evaluation

Depending on the experiment, cell cycle related influence may contribute to uninteresting variation in clustering. As a result, we can choose to regress the influence of cell cycle related genes in clustering.

We will look at the S Phase and G2 Phase scores separately. Additionally, we can separate all cycling cells (S + G2 phase) cells from all non-cycling cells by subtracting the G2M and S scores. 

```{r cell cycle calling}
#cell cyclce scoring
so <- lapply(X = so, FUN = function(x) {
  x <- CellCycleScoring(x, s.features = cc.genes.updated.2019$s.genes, g2m.features = cc.genes.updated.2019$g2m.genes)
})

for (i in names(so)) {
  so[[i]]$cc.difference <- so[[i]]$S.Score - so[[i]]$G2M.Score
  super_title <- ggdraw() +
  draw_text(text = i, x = 0.5, y = 1, size = 16)
  p <- VlnPlot(so[[i]], features = c('S.Score','G2M.Score','cc.difference'), group.by = "channel") + super_title & xlab(NULL)
  plot(p)
}
```

We can see there is little influence of these PBMC cells. We can always choose to regress this influence out to our choosing when selecting vars.to.regress during SCTransform. For now, we won't regress cell cycle genes from the data because there could be some interesting variation (i.e., proliferating cell populations).

## Modeling dropouts for significantly variable genes

Typically, highly variables genes (HVGs) are modeled through related the mean expression of a particular gene to its variance across all cells. This ensures that all subsequent downstream analysis only considers biologically relevant genes to remove the noise. However, unsupervised strategies to select for variable genes is rather challenging for single-cell experiments due to the low detection rates and technical variability (Tallulah S Andrews, Martin Hemberg, 2018).

Alternatively, the dropout-rates per genes can be correlated with mean gene expression levels for feature selection. The benefit of this method is that is it less sensitive to sampling noise. Here, we are looking for presence and absence of a gene which is rather binary in nature as opposed to variance.

```{r dropout modeling}
#load the necessary packages
library(M3Drop)
library(Matrix)

raw_counts <- list()
NBDropFS <- list()
for (i in names(so)) {
  # For memory purposes we are going to split each sample by their sample_id # this will also help us to observe some technical variation between captures
  raw_counts[[i]] <- SplitObject(so[[i]], split.by = "sample_id")
  
  message("completed splitting object")
  
  #retreive counts
  raw_counts[[i]] <- lapply(raw_counts[[i]], function(x) {
    x <- GetAssayData(x, slot = "counts", assay = "RNA")
  })
  
  message("completed retrieving counts data")
  
  #convert data and remove undetected genes
  raw_counts[[i]] <- lapply(raw_counts[[i]], function(x) {
  x <- NBumiConvertData(x, is.counts = TRUE)
  })
  
  message("completed removing undetected genes")
  
  #fit to the negative binomial model for UMI counts data
  raw_counts[[i]] <- lapply(raw_counts[[i]], function(x) {
  x <- NBumiFitModel(as.matrix(x))
  })
  
  message("completed fitting to NB model")
  
  #feature selection
  NBDropFS[[i]] <- lapply(raw_counts[[i]], function(x) {
  x <- NBumiFeatureSelectionCombinedDrop(x, method="fdr", qval.thres=0.01, suppress.plot=T) # the q.val threshold can be changed as needed
  })
  
  message("finished selecting features")
}
#the raw counts object can now be removed
rm(raw_counts)
```


```{r plot gene dropouts}

#find common features between all replicates
dropout_genes <- list()
features_selected <- list()
for (i in names(NBDropFS)) {
  for (p in names(NBDropFS[[i]])) {
    dropout_genes[[i]][[p]] <- NBDropFS[[i]][[p]]$Gene
  }
  features_selected[[i]] <- Reduce(intersect, dropout_genes[[i]])
}

#remove any targeted genes that may have shown up in feature selection
features_selected[[2]] <- setdiff(features_selected[[2]], targets)

#plot the dropout genes
dropout_df <- list()
for (i in names(so)) {
  counts <- so[[i]]@assays$RNA@counts
  dropout_df[[i]] <- data.frame(dropout_rate = 1 - (rowSums(counts > 0) / ncol(counts)),
                                mean_exp = rowMeans(counts),
                                features_selected = ifelse(rownames(counts) %in% features_selected[[i]], yes = TRUE, no = FALSE))
  p <- ggplot(dropout_df[[i]], aes(x = mean_exp, y = dropout_rate, color = features_selected)) +
    geom_point() +
    scale_x_log10() +
    scale_color_manual(values = c("#999999", "#E69F00")) +
    ggtitle(i)
  plot(p)
}
```

As we can see, we are selected for genes that have significant dropouts relative to their expression levels across all cells.

Now that we have selected for significant genes, we can proceed to dimensionality reduction techniques native to the Seurat toolkit

# Cell clustering using SCTransform workflow

```{r SCTransform}

#SCTransform
for (i in names(so)) {
  so[[i]] <- SCTransform(so[[i]], verbose = F, residual.features = features_selected[[i]])
  message("finished sctransform...")
}

#PCA
so <- lapply(X = so, FUN = function(x) {
  x <- RunPCA(x, assay = "SCT")
})

#generate UMAP coordinates
so <- lapply(X = so, FUN = function(x) {
  x <- RunUMAP(x, dims = 1:50, n.neighbors=30)
})

#find k-nearest neighbors
so <- lapply(X = so, FUN = function(x) {
  x <- FindNeighbors(x, dims = 1:50, k.param=10)
})

#Find clusters using the SLM algorithm which is most similar to leiden
so <- lapply(X = so, FUN = function(x) {
  x <- FindClusters(x, resolution = 1, algorithm = 3, verbose=T)
})

```

```{r Dimplots}

#Dimplots
for (i in names(so)) {
  dim <- DimPlot(so[[i]], reduction = "umap", split.by = "depletion", label = T, label.size = 3, repel = T) + NoLegend()
  plot(dim)
  #look for technical variation
  dim2 <- DimPlot(so[[i]], reduction = "umap", group.by = "channel", label = F) + NoLegend()
  plot(dim2)
}
```

Here, we see the same number of clusters with DepleteX.

For PBMC samples, we find that including introns in the analysis (default for Cell Ranger =>v7) may resolve a similar number of cluster communities between a standard 10x v3.1 and DepleteX library. With prior versions of Cell Ranger ( <v7) which do not include introns by default, we have observed additional cluster communities in addition to cell types. 

Despite potential similarities in semi-unsupervised clustering, we will showecase how to resolve additional cell types using DepleteX as some communities may be representative of uninteresting sources of variation. We will further showcase this using CellTypist (C. DomÃ­nguez Conde et al. , 2022) which is an automated annotation tool to describe the relationship to clustering performance and distinguishing cell types. 

We can save the figures into .h5ad format to be compatible for further analysis with python based workflows.


```{r Save the files}
library(SeuratDisk)
for (i in names(so)) {
  # Generate a unique filename based on the loop iteration
  filename <- paste("jumpcode_pbmc_analysis_files/", "example_", i, ".h5Seurat", sep = "")
  if (file.exists(filename)){
    message("file already exists")
  } else {
    DefaultAssay(so[[i]]) <- "RNA"
    SaveH5Seurat(so[[i]], filename = filename)
    Convert(filename, dest = "h5ad")
    write.csv(so[[i]]@reductions$pca@cell.embeddings, file = paste("jumpcode_pbmc_analysis_files/","example_", i, "_pca_embeddings.csv", sep = ""), quote = F, row.names = F)
  }
}
```

